{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext as tt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print = pprint.PrettyPrinter().pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Using device = cuda:0'\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are picking the TREC dataset as a proxy for the text classification problem. This has ~5500 training examples with 7 classes (one class is heavily imbalanced compared to the other 6 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 3.75MB/s]\n",
      "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 1.47MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tt.datasets.TREC\n",
    "\n",
    "#fields are essentially columns in your dataset. they pack a lot of\n",
    "#convenient functionality in their definitions\n",
    "\n",
    "#we have two fields - one for the raw text and one for the labels\n",
    "text_field = tt.data.Field(sequential = True,\n",
    "                           lower = True,\n",
    "                           init_token = '<sos>',\n",
    "                           eos_token = '<eos>'\n",
    "                          )\n",
    "\n",
    "#can also use tt.data.LabelField (exactly the same as below)\n",
    "label_field = tt.data.Field(sequential = False,\n",
    "                            unk_token = None)\n",
    "\n",
    "#parse and split the data into train and test\n",
    "train, test = dataset.splits(text_field, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Train has 5452 examples'\n",
      "'Test has 500 examples'\n"
     ]
    }
   ],
   "source": [
    "print(f'Train has {len(train.examples)} examples')\n",
    "print(f'Test has {len(test.examples)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how',\n",
      " 'did',\n",
      " 'serfdom',\n",
      " 'develop',\n",
      " 'in',\n",
      " 'and',\n",
      " 'then',\n",
      " 'leave',\n",
      " 'russia',\n",
      " '?']\n",
      "'DESC'\n"
     ]
    }
   ],
   "source": [
    "print(train.examples[0].text)\n",
    "print(train.examples[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, unique_counts = np.unique([ex.label for ex in train.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ABBR', 86),\n",
      " ('DESC', 1162),\n",
      " ('ENTY', 1250),\n",
      " ('HUM', 1223),\n",
      " ('LOC', 835),\n",
      " ('NUM', 896)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(unique_labels, unique_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [01:31, 9.40MB/s]                               \n",
      "100%|█████████▉| 399536/400000 [00:11<00:00, 33234.41it/s]"
     ]
    }
   ],
   "source": [
    "text_field.build_vocab(train, vectors = 'glove.6B.100d', unk_init = torch.Tensor.normal_)\n",
    "label_field.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Train text has 8681 unique words'\n",
      "'Train label has 6 unique words'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 399536/400000 [00:30<00:00, 33234.41it/s]"
     ]
    }
   ],
   "source": [
    "print(f'Train text has {len(text_field.vocab)} unique words')\n",
    "print(f'Train label has {len(label_field.vocab)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ENTY', 'HUM', 'DESC', 'NUM', 'LOC', 'ABBR'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_field.vocab.stoi.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = tt.data.BucketIterator.splits((train,test), \n",
    "                                                      batch_size = BATCH_SIZE,                                                      \n",
    "                                                      device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [' '.join(t.text) for t in train.examples]\n",
    "train_labels = [t.label for t in train.examples]\n",
    "\n",
    "test_text = [' '.join(t.text) for t in test.examples]\n",
    "test_labels = [t.label for t in test.examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(np.array(train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_tfidf = tfidf.transform(train_text)\n",
    "test_text_tfidf = tfidf.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=10, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=10, max_depth=50)\n",
    "rf.fit(train_text_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_tfidf = rf.predict(train_text_tfidf)\n",
    "test_pred_tfidf = rf.predict(test_text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Train Acc = 0.9180117388114454'\n",
      "'Test  Acc = 0.82'\n"
     ]
    }
   ],
   "source": [
    "train_acc_tfidf = np.sum(train_pred_tfidf == train_labels) / len(train_labels)\n",
    "test_acc_tfidf = np.sum(test_pred_tfidf == test_labels) / len(test_labels)\n",
    "\n",
    "print(f'Train Acc = {train_acc_tfidf}')\n",
    "print(f'Test  Acc = {test_acc_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, #vocab size\n",
    "                 embed_dim, #user defined\n",
    "                 hidden_dim, #user defined\n",
    "                 output_dim, #number of output classes\n",
    "                 device = None\n",
    "                ):\n",
    "        \n",
    "        super(TextClassification, self).__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        #can pick LSTM, GRU etc.\n",
    "        num_layers = 5\n",
    "        bidirectional = True\n",
    "\n",
    "        if bidirectional: n_directions = 2\n",
    "        else: n_directions = 1\n",
    "\n",
    "\n",
    "        \n",
    "        '''\n",
    "        self.rnn = nn.RNN(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = num_layers,\n",
    "                          nonlinearity = 'relu',\n",
    "                          bias = True,\n",
    "                          batch_first = False,\n",
    "                          dropout = 0.5,\n",
    "                          bidirectional = bidirectional)\n",
    "        '''\n",
    "        self.rnn = nn.GRU(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = num_layers,\n",
    "                          #nonlinearity = 'relu',\n",
    "                          bias = True,\n",
    "                          batch_first = False,\n",
    "                          dropout = 0.5,\n",
    "                          bidirectional = bidirectional)\n",
    "        \n",
    "        #output is a linear layer (log probs of belonging to output classes)\n",
    "        self.output = nn.Linear(num_layers * n_directions * hidden_dim, output_dim)\n",
    "        \n",
    "        if device is not None: self.to(device)\n",
    "        \n",
    "    def forward(self, string_numerical):\n",
    "        embed_batch = self.embedding(string_numerical)\n",
    "        \n",
    "        output, hidden = self.rnn(embed_batch)\n",
    "        \n",
    "        log_prob = self.output(hidden.permute(1,0,2).flatten(start_dim=1, end_dim=2)).squeeze(0)\n",
    "        \n",
    "        #log_prob = self.output(hidden).squeeze(0)\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, #vocab size\n",
    "                 embed_dim, #user defined\n",
    "                 hidden_dim, #user defined\n",
    "                 output_dim, #number of output classes\n",
    "                 #bidirectional = False,\n",
    "                 device = None\n",
    "                ):\n",
    "        \n",
    "        super(TextClassificationAttention, self).__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        #can pick LSTM, GRU etc.\n",
    "        '''\n",
    "        self.rnn = nn.RNN(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = 1,\n",
    "                          nonlinearity = 'relu',\n",
    "                          bias = True,\n",
    "                          batch_first = False,\n",
    "                          dropout = 0.5,\n",
    "                          bidirectional = False)\n",
    "        '''\n",
    "        \n",
    "        num_layers = 2\n",
    "        bidirectional = True\n",
    "        if bidirectional: n_directions = 2\n",
    "        else: n_directions = 1\n",
    "\n",
    "        self.rnn = nn.GRU(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = num_layers,\n",
    "                          #nonlinearity = 'relu',\n",
    "                          bias = True,\n",
    "                          batch_first = False,\n",
    "                          dropout = 0.5,\n",
    "                          bidirectional = bidirectional)\n",
    "        \n",
    "        #output is a linear layer (log probs of belonging to output classes)\n",
    "        self.output = nn.Linear(num_layers * n_directions * hidden_dim, output_dim)\n",
    "        \n",
    "        #attention specific layers\n",
    "        self.attention_linear = nn.Linear(n_directions * hidden_dim, 1)\n",
    "        self.attention_softmax = nn.Softmax(dim=0)        \n",
    "        \n",
    "        if device is not None: self.to(device)\n",
    "        \n",
    "    def forward(self, string_numerical):\n",
    "        embed_batch = self.embedding(string_numerical)\n",
    "        \n",
    "        output, hidden = self.rnn(embed_batch)\n",
    "        \n",
    "        attention_weights = self.attention_softmax(self.attention_linear(output).squeeze(2))\n",
    "        attention_hidden = (attention_weights.unsqueeze(0).permute(1,2,0).expand_as(output) * output).sum(dim=0)\n",
    "                \n",
    "        #log_prob = self.output(attention_hidden).squeeze(0)\n",
    "        log_prob = self.output(attention_hidden.permute(1,0,2).flatten(start_dim=1, end_dim=2)).squeeze(0)\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Check data flows through model without problems\n",
    "\n",
    "This section is used to ensure the forward flow makes sense (dimensionally). Ignore for training new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Looking at first example...\\n'\n",
      "['how',\n",
      " 'did',\n",
      " 'serfdom',\n",
      " 'develop',\n",
      " 'in',\n",
      " 'and',\n",
      " 'then',\n",
      " 'leave',\n",
      " 'russia',\n",
      " '?']\n"
     ]
    }
   ],
   "source": [
    "print('Looking at first example...\\n')\n",
    "string = train.examples[0].text\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Numericalize i.e. replace word by index in vocab...\\n'\n",
      "[11, 23, 7662, 2536, 9, 19, 509, 866, 1160, 4]\n"
     ]
    }
   ],
   "source": [
    "print('Numericalize i.e. replace word by index in vocab...\\n')\n",
    "string_numerical = [text_field.vocab.stoi[word] for word in string]\n",
    "print(string_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Fields have function numericalize to do the same thing...\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  11],\n",
       "        [  23],\n",
       "        [7662],\n",
       "        [2536],\n",
       "        [   9],\n",
       "        [  19],\n",
       "        [ 509],\n",
       "        [ 866],\n",
       "        [1160],\n",
       "        [   4]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fields have function numericalize to do the same thing...\\n')\n",
    "text_field.numericalize([string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter.create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The model works with batches of text and corresponding labels...\\n'\n",
      "'Batch size = 32'\n"
     ]
    }
   ],
   "source": [
    "print('The model works with batches of text and corresponding labels...\\n')\n",
    "string_batch = next(train_iter.batches)\n",
    "print(f'Batch size = {len(string_batch)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Separate text and labels in each batch\\n'\n"
     ]
    }
   ],
   "source": [
    "print('Separate text and labels in each batch\\n')\n",
    "string_batch_text = [s.text for s in string_batch]\n",
    "string_batch_label = [s.label for s in string_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Each field has process function to do preprocessing, numericalization and '\n",
      " 'post-processing before creating a batch\\n')\n",
      "('Flow = Data -> Tokenize -> Preprocess Func -> Numericalize -> Example '\n",
      " 'Instance -> Postprocess -> Batch Instance\\n')\n",
      "tensor([   2,   49,   17,   51,  315,   27,    5,  774,    4,   49,   17,   51,\n",
      "         359, 3124,   89, 2177,    4,    3,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "print('Each field has process function to do preprocessing, numericalization and post-processing before creating a batch\\n')\n",
    "print('Flow = Data -> Tokenize -> Preprocess Func -> Numericalize -> Example Instance -> Postprocess -> Batch Instance\\n')\n",
    "\n",
    "print(text_field.process(string_batch_text)[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Size of Text vocabulary...'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8681"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Size of Text vocabulary...')\n",
    "len(text_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Define dimensions...'\n"
     ]
    }
   ],
   "source": [
    "print('Define dimensions...')\n",
    "input_dim = len(text_field.vocab)\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "bidirectional = True\n",
    "if bidirectional: n_directions = 2\n",
    "else: n_directions = 1\n",
    "\n",
    "embedding = nn.Embedding(input_dim, embed_dim)\n",
    "rnn = nn.RNN(input_size = embed_dim,\n",
    "                          hidden_size = hidden_dim,\n",
    "                          num_layers = num_layers,\n",
    "                          nonlinearity = 'relu',\n",
    "                          bias = True,\n",
    "                          batch_first = False,\n",
    "                          dropout = 0.5,\n",
    "                          bidirectional = bidirectional)\n",
    "output = nn.Linear(num_layers * n_directions * hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Step 1: Numericalize strings...'\n",
      "torch.Size([20, 32])\n"
     ]
    }
   ],
   "source": [
    "print('Step 1: Numericalize strings...')\n",
    "string_batch_numerical = text_field.process(string_batch_text)\n",
    "print(string_batch_numerical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Step 2: Embed tokens...'\n",
      "torch.Size([20, 32, 100])\n"
     ]
    }
   ],
   "source": [
    "print('Step 2: Embed tokens...')\n",
    "embed_batch = embedding(string_batch_numerical)\n",
    "print(embed_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Step 3: Pass through rnn...'\n",
      "torch.Size([20, 32, 128])\n",
      "torch.Size([1, 32, 128])\n",
      "'Note: hidden tensor size = (num_layers * n_directions, batch_size, hidden_dim)'\n"
     ]
    }
   ],
   "source": [
    "print('Step 3: Pass through rnn...')\n",
    "rnn_batch_output, rnn_batch_hidden = rnn(embed_batch)\n",
    "print(rnn_batch_output.shape)\n",
    "print(rnn_batch_hidden.shape)\n",
    "print('Note: hidden tensor size = (num_layers * n_directions, batch_size, hidden_dim)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_prob = output(rnn_batch_hidden.permute(1,0,2).flatten(start_dim=1, end_dim=2)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "print(model_log_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 2, 0, 3, 0, 0, 3, 1, 4, 2, 2, 0, 1, 0, 1, 3, 1, 3, 2, 2, 3, 0, 2,\n",
       "        1, 2, 4, 0, 1, 0, 0, 3])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_field.process(string_batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0242, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "string_batch_numerical_label = label_field.process(string_batch_label)\n",
    "loss = criterion(model_log_prob, string_batch_numerical_label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Multiple layers\n",
    "\n",
    "This section is used to ensure the forward flow makes sense (dimensionally). Ignore for training new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_batch_numerical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 128])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_batch_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 128])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_batch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [640 x 128], m2: [256 x 1] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-317-e2b513868b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_batch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [640 x 128], m2: [256 x 1] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:961"
     ]
    }
   ],
   "source": [
    "weights = nn.Softmax(dim=0)(nn.Linear(2*128, 1)(rnn_batch_output).squeeze(2))\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_batch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_batch_output[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(weights[:,0], rnn_batch_output[:,0,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights.unsqueeze(0).permute(1,2,0).expand_as(rnn_batch_output) * rnn_batch_output).sum(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32])\n",
      "torch.Size([20, 32, 100])\n",
      "torch.Size([20, 32, 256])\n",
      "torch.Size([4, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "print(string_batch_numerical.shape)\n",
    "\n",
    "embed_batch = embedding(string_batch_numerical)\n",
    "print(embed_batch.shape)\n",
    "\n",
    "output, hidden = rnn(embed_batch)\n",
    "print(output.shape)\n",
    "print(hidden.shape)\n",
    "\n",
    "#attention_weights = nn.Softmax(dim=0)(nn.Linear(num_layers * n_directions, 1)(output).squeeze(2))\n",
    "#attention_hidden = (attention_weights.unsqueeze(0).permute(1,2,0).expand_as(output) * output).sum(dim=0)\n",
    "\n",
    "#log_prob = self.output(attention_hidden).squeeze(0)\n",
    "#log_prob = self.output(attention_hidden.permute(1,0,2).flatten(start_dim=1, end_dim=2)).squeeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for accuracy\n",
    "def accuracy(pred, labels):\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    acc = (pred_classes==labels).float().mean()\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, text_field, label_field, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for string_batch in iterator:\n",
    "        #separate text and label (already numericalized)\n",
    "        string_batch_text = string_batch.text\n",
    "        string_batch_label = string_batch.label\n",
    "                \n",
    "        #evaluate loss\n",
    "        pred_log_prob = model(string_batch_text)\n",
    "        loss = criterion(pred_log_prob, string_batch_label)\n",
    "        \n",
    "        #gradient descent (Adam)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "                \n",
    "        #accuracy\n",
    "        acc = accuracy(pred_log_prob, string_batch_label)\n",
    "        accuracy_list.append(acc.item())\n",
    "\n",
    "    total_loss /= len(iterator)\n",
    "        \n",
    "    return model, total_loss, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, iterator, text_field, label_field):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_pred = 0\n",
    "    \n",
    "    with torch.no_grad(): #don't compute gradients\n",
    "        for string_batch in iterator:\n",
    "            #separate text and label\n",
    "            string_batch_text = string_batch.text\n",
    "            string_batch_label = string_batch.label\n",
    "                                    \n",
    "            #evaluate loss\n",
    "            pred_log_prob = model(string_batch_text)\n",
    "            loss = criterion(pred_log_prob, string_batch_label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_classes = torch.argmax(pred_log_prob, dim=1)\n",
    "            correct_pred += (pred_classes==string_batch_label).float().sum()\n",
    "            \n",
    "    acc = correct_pred / len(test)\n",
    "    \n",
    "    total_loss /= len(iterator)\n",
    "    \n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train Loss = 0.993 Median Train Batch Accuracy = 0.62 Test Loss = 0.729 Test '\n",
      " 'Accuracy = 0.70')\n",
      "('Train Loss = 0.636 Median Train Batch Accuracy = 0.78 Test Loss = 0.520 Test '\n",
      " 'Accuracy = 0.83')\n",
      "('Train Loss = 0.471 Median Train Batch Accuracy = 0.84 Test Loss = 0.468 Test '\n",
      " 'Accuracy = 0.86')\n",
      "('Train Loss = 0.331 Median Train Batch Accuracy = 0.88 Test Loss = 0.699 Test '\n",
      " 'Accuracy = 0.73')\n",
      "('Train Loss = 0.227 Median Train Batch Accuracy = 0.94 Test Loss = 0.699 Test '\n",
      " 'Accuracy = 0.75')\n",
      "('Train Loss = 0.143 Median Train Batch Accuracy = 0.97 Test Loss = 0.628 Test '\n",
      " 'Accuracy = 0.85')\n",
      "('Train Loss = 0.105 Median Train Batch Accuracy = 0.97 Test Loss = 0.688 Test '\n",
      " 'Accuracy = 0.83')\n",
      "('Train Loss = 0.067 Median Train Batch Accuracy = 1.00 Test Loss = 0.725 Test '\n",
      " 'Accuracy = 0.85')\n",
      "('Train Loss = 0.045 Median Train Batch Accuracy = 1.00 Test Loss = 0.853 Test '\n",
      " 'Accuracy = 0.83')\n",
      "('Train Loss = 0.034 Median Train Batch Accuracy = 1.00 Test Loss = 0.863 Test '\n",
      " 'Accuracy = 0.83')\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(text_field.vocab)\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = len(unique_labels)\n",
    "\n",
    "N_epochs = 10\n",
    "\n",
    "model = TextClassification(input_dim, embed_dim, hidden_dim, output_dim, device)\n",
    "#model = TextClassificationAttention(input_dim, embed_dim, hidden_dim, output_dim, device)\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    model, loss, accuracy_list = train_one_epoch(model, train_iter, text_field, label_field, lr=1e-3)\n",
    "    \n",
    "    test_loss, test_acc = validate(model, test_iter, text_field, label_field)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Train Loss = {loss:.3f} Median Train Batch Accuracy = {np.median(accuracy_list):.2f} Test Loss = {test_loss:.3f} Test Accuracy = {test_acc:.2f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassification(\n",
       "  (embedding): Embedding(8681, 100)\n",
       "  (rnn): GRU(100, 128, dropout=0.5)\n",
       "  (output): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2331, -4.7929,  8.4943, -1.6038, -2.8070, -3.0430],\n",
       "        [ 6.4501, -0.3446, -2.3021, -4.4463, -3.0333, -2.7899],\n",
       "        [-0.3929, -5.2317,  8.7567, -2.3056, -1.8869, -2.7242],\n",
       "        ...,\n",
       "        [ 1.6309, -5.4118,  1.5285,  9.7031, -5.6745, -2.0729],\n",
       "        [ 1.5351, -4.5882,  0.7383, 10.1366, -5.7996, -2.5140],\n",
       "        [ 5.9539, -2.8718, -2.0939, -1.5626, -1.4098, -3.5202]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(text_field.process([s.text for s in train.examples]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions on all test examples to confirm 84% accuracy above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_classes = torch.argmax(model(text_field.process([s.text for s in train.examples]).to(device)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = label_field.process([s.label for s in train.examples]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860601614086574"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_pred_classes==train_label).sum().item() / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_classes = torch.argmax(model(text_field.process([s.text for s in test.examples]).to(device)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = label_field.process([s.label for s in test.examples]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_pred_classes==test_label).sum().item() / len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Precision and Recall for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision, train_recall, train_f1, train_support = precision_recall_fscore_support(train_label.cpu().detach().numpy(), train_pred_classes.cpu().detach().numpy())\n",
    "test_precision, test_recall, test_f1, test_support = precision_recall_fscore_support(test_label.cpu().detach().numpy(), test_pred_classes.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'], dtype='<U4'),\n",
      " array([  86, 1162, 1250, 1223,  835,  896]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>N_examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>0.976172</td>\n",
       "      <td>0.983200</td>\n",
       "      <td>0.979673</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUM</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.976288</td>\n",
       "      <td>0.986777</td>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESC</td>\n",
       "      <td>0.977215</td>\n",
       "      <td>0.996558</td>\n",
       "      <td>0.986792</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995556</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.992832</td>\n",
       "      <td>0.995210</td>\n",
       "      <td>0.994019</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  precision    recall        f1  N_examples\n",
       "0  ENTY   0.976172  0.983200  0.979673        1250\n",
       "1   HUM   0.997494  0.976288  0.986777        1223\n",
       "2  DESC   0.977215  0.996558  0.986792        1162\n",
       "3   NUM   0.991150  1.000000  0.995556         896\n",
       "4   LOC   0.992832  0.995210  0.994019         835\n",
       "5  ABBR   0.971429  0.790698  0.871795          86"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics = pd.DataFrame({'class': [label_field.vocab.itos[i] for i in range(6)],\n",
    "                              'precision': train_precision,\n",
    "                              'recall': train_recall,\n",
    "                              'f1': train_f1,\n",
    "                              'N_examples': train_support})\n",
    "\n",
    "print(np.unique([ex.label for ex in train.examples], return_counts=True))\n",
    "\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'], dtype='<U4'),\n",
      " array([  9, 138,  94,  65,  81, 113]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>N_examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUM</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DESC</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.886447</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>0.882591</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  precision    recall        f1  N_examples\n",
       "0  ENTY   0.804348  0.787234  0.795699          94\n",
       "1   HUM   0.963636  0.815385  0.883333          65\n",
       "2  DESC   0.896296  0.876812  0.886447         138\n",
       "3   NUM   0.813433  0.964602  0.882591         113\n",
       "4   LOC   0.883117  0.839506  0.860759          81\n",
       "5  ABBR   1.000000  0.777778  0.875000           9"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics = pd.DataFrame({'class': [label_field.vocab.itos[i] for i in range(6)],\n",
    "                             'precision': test_precision,\n",
    "                             'recall': test_recall,\n",
    "                             'f1': test_f1,\n",
    "                             'N_examples': test_support})\n",
    "\n",
    "print(np.unique([ex.label for ex in test.examples], return_counts=True))\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sanjay/BrnoTeaching2019/notebooks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
